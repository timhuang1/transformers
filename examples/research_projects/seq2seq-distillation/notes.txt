================================
all arguments and description for model fine-tuning 

optional arguments:
  -h, --help            show this help message and exit
  --logger [LOGGER]     autogenerated by pl.Trainer
  --checkpoint_callback [CHECKPOINT_CALLBACK]
                        autogenerated by pl.Trainer
  --early_stop_callback [EARLY_STOP_CALLBACK]
                        autogenerated by pl.Trainer
  --default_root_dir DEFAULT_ROOT_DIR
                        autogenerated by pl.Trainer
  --gradient_clip_val GRADIENT_CLIP_VAL
                        autogenerated by pl.Trainer
  --process_position PROCESS_POSITION
                        autogenerated by pl.Trainer
  --num_nodes NUM_NODES
                        autogenerated by pl.Trainer
  --num_processes NUM_PROCESSES
                        autogenerated by pl.Trainer
  --gpus GPUS           autogenerated by pl.Trainer
  --auto_select_gpus [AUTO_SELECT_GPUS]
                        autogenerated by pl.Trainer
  --tpu_cores TPU_CORES
                        autogenerated by pl.Trainer
  --log_gpu_memory LOG_GPU_MEMORY
                        autogenerated by pl.Trainer
  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE
                        autogenerated by pl.Trainer
  --overfit_batches OVERFIT_BATCHES
                        autogenerated by pl.Trainer
  --track_grad_norm TRACK_GRAD_NORM
                        autogenerated by pl.Trainer
  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH
                        autogenerated by pl.Trainer
  --fast_dev_run [FAST_DEV_RUN]
                        autogenerated by pl.Trainer
  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES
                        autogenerated by pl.Trainer
  --max_epochs MAX_EPOCHS
                        autogenerated by pl.Trainer
  --min_epochs MIN_EPOCHS
                        autogenerated by pl.Trainer
  --max_steps MAX_STEPS
                        autogenerated by pl.Trainer
  --min_steps MIN_STEPS
                        autogenerated by pl.Trainer
  --limit_train_batches LIMIT_TRAIN_BATCHES
                        autogenerated by pl.Trainer
  --limit_val_batches LIMIT_VAL_BATCHES
                        autogenerated by pl.Trainer
  --limit_test_batches LIMIT_TEST_BATCHES
                        autogenerated by pl.Trainer
  --val_check_interval VAL_CHECK_INTERVAL
                        autogenerated by pl.Trainer
  --log_save_interval LOG_SAVE_INTERVAL
                        autogenerated by pl.Trainer
  --row_log_interval ROW_LOG_INTERVAL
                        autogenerated by pl.Trainer
  --distributed_backend DISTRIBUTED_BACKEND
                        autogenerated by pl.Trainer
  --sync_batchnorm [SYNC_BATCHNORM]
                        autogenerated by pl.Trainer
  --precision PRECISION
                        autogenerated by pl.Trainer
  --weights_summary WEIGHTS_SUMMARY
                        autogenerated by pl.Trainer
  --weights_save_path WEIGHTS_SAVE_PATH
                        autogenerated by pl.Trainer
  --num_sanity_val_steps NUM_SANITY_VAL_STEPS
                        autogenerated by pl.Trainer
  --truncated_bptt_steps TRUNCATED_BPTT_STEPS
                        autogenerated by pl.Trainer
  --resume_from_checkpoint RESUME_FROM_CHECKPOINT
                        autogenerated by pl.Trainer
  --profiler [PROFILER]
                        autogenerated by pl.Trainer
  --benchmark [BENCHMARK]
                        autogenerated by pl.Trainer
  --deterministic [DETERMINISTIC]
                        autogenerated by pl.Trainer
  --reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]
                        autogenerated by pl.Trainer
  --auto_lr_find [AUTO_LR_FIND]
                        autogenerated by pl.Trainer
  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]
                        autogenerated by pl.Trainer
  --terminate_on_nan [TERMINATE_ON_NAN]
                        autogenerated by pl.Trainer
  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]
                        autogenerated by pl.Trainer
  --prepare_data_per_node [PREPARE_DATA_PER_NODE]
                        autogenerated by pl.Trainer
  --amp_backend AMP_BACKEND
                        autogenerated by pl.Trainer
  --amp_level AMP_LEVEL
                        autogenerated by pl.Trainer
  --val_percent_check VAL_PERCENT_CHECK
                        autogenerated by pl.Trainer
  --test_percent_check TEST_PERCENT_CHECK
                        autogenerated by pl.Trainer
  --train_percent_check TRAIN_PERCENT_CHECK
                        autogenerated by pl.Trainer
  --overfit_pct OVERFIT_PCT
                        autogenerated by pl.Trainer
  --model_name_or_path MODEL_NAME_OR_PATH
                        Path to pretrained model or model identifier from
                        huggingface.co/models
  --config_name CONFIG_NAME
                        Pretrained config name or path if not the same as
                        model_name
  --tokenizer_name TOKENIZER_NAME
                        Pretrained tokenizer name or path if not the same as
                        model_name
  --cache_dir CACHE_DIR
                        Where do you want to store the pre-trained models
                        downloaded from s3
  --encoder_layerdrop ENCODER_LAYERDROP
                        Encoder layer dropout probability (Optional). Goes
                        into model.config
  --decoder_layerdrop DECODER_LAYERDROP
                        Decoder layer dropout probability (Optional). Goes
                        into model.config
  --dropout DROPOUT     Dropout probability (Optional). Goes into model.config
  --attention_dropout ATTENTION_DROPOUT
                        Attention dropout probability (Optional). Goes into
                        model.config
  --learning_rate LEARNING_RATE
                        The initial learning rate for Adam.
  --lr_scheduler {cosine, cosine_w_restarts, linear, polynomial}
                        Learning rate scheduler
  --weight_decay WEIGHT_DECAY
                        Weight decay if we apply some.
  --adam_epsilon ADAM_EPSILON
                        Epsilon for Adam optimizer.
  --warmup_steps WARMUP_STEPS
                        Linear warmup over warmup_steps.
  --num_workers NUM_WORKERS
                        kwarg passed to DataLoader
  --num_train_epochs MAX_EPOCHS
  --train_batch_size TRAIN_BATCH_SIZE
  --eval_batch_size EVAL_BATCH_SIZE
  --adafactor
  --output_dir OUTPUT_DIR
                        The output directory where the model predictions and
                        checkpoints will be written.
  --fp16                Whether to use 16-bit (mixed) precision (through
                        NVIDIA apex) instead of 32-bit
  --fp16_opt_level FP16_OPT_LEVEL
                        For fp16: Apex AMP optimization level selected in
                        ['O0', 'O1', 'O2', and 'O3'].See details at
                        https://nvidia.github.io/apex/amp.html
  --n_tpu_cores TPU_CORES
  --max_grad_norm GRADIENT_CLIP_VAL
                        Max gradient norm
  --do_train            Whether to run training.
  --do_predict          Whether to run predictions on the test set.
  --gradient_accumulation_steps ACCUMULATE_GRAD_BATCHES
                        Number of updates steps to accumulate before
                        performing a backward/update pass.
  --seed SEED           random seed for initialization
  --data_dir DATA_DIR   The input data dir. Should contain the training files
                        for the CoNLL-2003 NER task.
  --max_source_length MAX_SOURCE_LENGTH
                        The maximum total input sequence length after
                        tokenization. Sequences longer than this will be
                        truncated, sequences shorter will be padded.
  --max_target_length MAX_TARGET_LENGTH
                        The maximum total input sequence length after
                        tokenization. Sequences longer than this will be
                        truncated, sequences shorter will be padded.
  --val_max_target_length VAL_MAX_TARGET_LENGTH
                        The maximum total input sequence length after
                        tokenization. Sequences longer than this will be
                        truncated, sequences shorter will be padded.
  --test_max_target_length TEST_MAX_TARGET_LENGTH
                        The maximum total input sequence length after
                        tokenization. Sequences longer than this will be
                        truncated, sequences shorter will be padded.
  --freeze_encoder
  --freeze_embeds
  --sortish_sampler
  --max_tokens_per_batch MAX_TOKENS_PER_BATCH
  --logger_name {default,wandb,wandb_shared}
  --n_train N_TRAIN     # examples. -1 means use all.
  --n_val N_VAL         # examples. -1 means use all.
  --n_test N_TEST       # examples. -1 means use all.
  --task TASK           # examples. -1 means use all.
  --label_smoothing LABEL_SMOOTHING
  --src_lang SRC_LANG
  --tgt_lang TGT_LANG
  --eval_beams EVAL_BEAMS
  --val_metric {bleu,rouge2,loss,None}
  --eval_max_gen_length EVAL_MAX_GEN_LENGTH
                        never generate more than n tokens
  --save_top_k SAVE_TOP_K
                        How many checkpoints to save
  --early_stopping_patience EARLY_STOPPING_PATIENCE
                        -1 means never early stop. early_stopping_patience is
                        measured in validation checks, not epochs. So
                        val_check_interval will effect it.




================================
Commands for evaluation
export DATA_DIR=cnn_dm
./run_eval.py cnn_results/best_tfmr $DATA_DIR/val.source bart_val_generations.txt \
    --reference_path $DATA_DIR/val.target \
    --score_path cnn_rouge.json \
    --task summarization \
    --n_obs 100 \
    --device cuda \
    --max_source_length 1024 \
    --max_target_length 56 \
    --bs 32








